{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76122136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charger_modele(lien_model=\"poeme_generator_74_20_pourcent.h5\"):\n",
    "    \n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    \n",
    "    model_70_63_percent = load_model(lien_model)\n",
    "    \n",
    "    return (tf, Tokenizer, pad_sequences, np, model_70_63_percent) \n",
    "\n",
    "tf, Tokenizer, pad_sequences, np, model = charger_modele()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725429d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(lien_fichier='./../data/poeme.txt', tokenizer=Tokenizer(), pad_sequences=pad_sequences):\n",
    "    \n",
    "    # Charger le jeu de données\n",
    "    data = open(lien_fichier, \"r\",  encoding='utf-8').read()\n",
    "\n",
    "    # Mettre en minuscules et diviser le texte\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # Initialiser la classe Tokenizer\n",
    "    #tokenizer = Tokenizer()\n",
    "\n",
    "    # Générer le dictionnaire d'index de mots\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Initialiser la liste des séquences\n",
    "    input_sequences = []\n",
    "\n",
    "    # Boucle sur chaque ligne\n",
    "    for line in corpus:\n",
    "\n",
    "        # Tokeniser la ligne courante\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "        # Bouclez plusieurs fois sur la ligne pour générer les sous-phrases\n",
    "        for i in range(1, len(token_list)):\n",
    "\n",
    "            # Générer la sous-expression\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "            # Ajouter la sous-phrase à la liste des séquences\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # Obtenir la longueur de la ligne la plus longue\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "    # Pad toutes les séquences\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # Créez des entrées et une étiquette en divisant le dernier jeton dans les sous-phrases\n",
    "    xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "    # Convertir l'étiquette en tableaux one-hot\n",
    "    ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    return tokenizer, max_sequence_len, token_list, xs, ys\n",
    "\n",
    "tokenizer, max_sequence_len, token_list, xs, ys = data_preprocessing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"Un poème\"\n",
    "next_words = 100\n",
    "\n",
    "def generer_poeme(seed_text, next_words=next_words, token_list=token_list, model=model, tokenizer=tokenizer, max_sequence_len=max_sequence_len):\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # Convert the text into sequences\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Pad the sequences\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        # Get the probabilities of predicting a word\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        # Choose the next word based on the maximum probability\n",
    "        predicted = np.argmax(predicted, axis=-1).item()\n",
    "        # Get the actual word from the word index\n",
    "        output_word = tokenizer.index_word[predicted]\n",
    "        # Append to the current text\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78dda43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"aimer le vent baigner ma tête je l'aime quand e et la nuque baignant dans le vent de l’été divin perdu le seul regard l'éclaircirait car je te plonger ses fruits mes meilleurs vœux de mes vers de bohème le doux que embelli vous laisse mes yeux mais je ne le veut vraiment pas puis qu'une telle fleur ne dure nous ne pouvons que vivre ensemble et de fruits tombés pour toi que cette silhouette des jours heureux des amoureux s'en va je t'aime ne peut l'avouer je tiens à composer tous vos pieds reposée elle est le nain micromégas pouvoir\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generer_poeme(seed_text=\"aimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc44fa04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aebdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
