{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbeaa619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charger_modele(lien_model=\"poeme_generator_70_63_pourcent.h5\"):\n",
    "    \n",
    "    from tensorflow.keras.models import load_model\n",
    "\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    import numpy as np\n",
    "    \n",
    "    model_70_63_percent = load_model(lien_model)\n",
    "    \n",
    "    return (tf, Tokenizer, pad_sequences, np, model_70_63_percent) \n",
    "\n",
    "tf, Tokenizer, pad_sequences, np, model = charger_modele()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af005ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(lien_fichier='./../data/poeme.txt', tokenizer=Tokenizer(), pad_sequences=pad_sequences):\n",
    "    \n",
    "    # Charger le jeu de données\n",
    "    data = open(lien_fichier, \"r\",  encoding='utf-8').read()\n",
    "\n",
    "    # Mettre en minuscules et diviser le texte\n",
    "    corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "    # Initialiser la classe Tokenizer\n",
    "    #tokenizer = Tokenizer()\n",
    "\n",
    "    # Générer le dictionnaire d'index de mots\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Initialiser la liste des séquences\n",
    "    input_sequences = []\n",
    "\n",
    "    # Boucle sur chaque ligne\n",
    "    for line in corpus:\n",
    "\n",
    "        # Tokeniser la ligne courante\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\n",
    "        # Bouclez plusieurs fois sur la ligne pour générer les sous-phrases\n",
    "        for i in range(1, len(token_list)):\n",
    "\n",
    "            # Générer la sous-expression\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "\n",
    "            # Ajouter la sous-phrase à la liste des séquences\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    # Obtenir la longueur de la ligne la plus longue\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "    # Pad toutes les séquences\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "    # Créez des entrées et une étiquette en divisant le dernier jeton dans les sous-phrases\n",
    "    xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "    # Convertir l'étiquette en tableaux one-hot\n",
    "    ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
    "\n",
    "    return token_list, xs, ys\n",
    "\n",
    "token_list, xs, ys = data_preprocessing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bb2a2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"aimer de l'autre côté rêveurs ange de tes vagues qui claquent avec force l'oreille qu'un sourire me dompte et m'adoucit passe et repasse de telle sorte un doux présage matinal et pourtant qu'a être fantôme intéressantes et emballantes n'est l'horizon qui nous donnait à tous ce pauvre me avenir meilleur quand tu me parais bien la porte flanqué depuis une peu si gracieuse comme un lion hors laissez de votre rivages en jeune fille ou bien s'il fait aimer votre âme en soit déclarée je ne veux fuir pas mes yeux que l'on croyait encore un peu d'envie et respecté cœur\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = \"Un poème\"\n",
    "next_words = 100\n",
    "\n",
    "def generer_poeme(seed_text, next_words=next_words, token_list=token_list, model=model):\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # Convert the text into sequences\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        # Pad the sequences\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        # Get the probabilities of predicting a word\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        # Choose the next word based on the maximum probability\n",
    "        predicted = np.argmax(predicted, axis=-1).item()\n",
    "        # Get the actual word from the word index\n",
    "        output_word = tokenizer.index_word[predicted]\n",
    "        # Append to the current text\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "generer_poeme(seed_text='aimer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4580dfd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8015018105506897"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
